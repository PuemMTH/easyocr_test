#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import pandas as pd
import numpy as np

def analyze_negative_accuracy():
    """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà character_accuracy ‡∏ï‡∏¥‡∏î‡∏•‡∏ö"""
    
    print("üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
    
    # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å CSV
    try:
        df = pd.read_csv('ocr_evaluation_detailed.csv')
        print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(df):,} ‡πÅ‡∏ñ‡∏ß")
    except FileNotFoundError:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå ocr_evaluation_detailed.csv")
        return
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
    print(f"\nüìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ:")
    print(f"   - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•: {df['model_name'].nunique()}")
    print(f"   - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {df['dataset_name'].nunique()}")
    print(f"   - ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {list(df.columns)}")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö CER values
    print(f"\nüìà ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ CER percent:")
    print(f"   - Min CER: {df['cer_percent'].min():.4f}%")
    print(f"   - Max CER: {df['cer_percent'].max():.4f}%")
    print(f"   - Mean CER: {df['cer_percent'].mean():.4f}%")
    print(f"   - Std CER: {df['cer_percent'].std():.4f}%")
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì character_accuracy
    df['character_accuracy_calculated'] = 100 - df['cer_percent']
    
    print(f"\nüßÆ ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ Character Accuracy (100 - CER):")
    print(f"   - Min Accuracy: {df['character_accuracy_calculated'].min():.4f}%")
    print(f"   - Max Accuracy: {df['character_accuracy_calculated'].max():.4f}%")
    print(f"   - Mean Accuracy: {df['character_accuracy_calculated'].mean():.4f}%")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏•‡∏ö
    negative_cases = df[df['character_accuracy_calculated'] < 0]
    
    if len(negative_cases) > 0:
        print(f"\n‚ö†Ô∏è  ‡∏û‡∏ö Character Accuracy ‡∏ï‡∏¥‡∏î‡∏•‡∏ö: {len(negative_cases):,} ‡∏Å‡∏£‡∏ì‡∏µ")
        print(f"   - ‡∏Ñ‡∏¥‡∏î‡πÄ‡∏õ‡πá‡∏ô {(len(negative_cases)/len(df)*100):.2f}% ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÅ‡∏¢‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        worst_cases = negative_cases.nsmallest(5, 'character_accuracy_calculated')
        
        print(f"\nüî¥ 5 ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÅ‡∏¢‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î:")
        for i, (idx, row) in enumerate(worst_cases.iterrows(), 1):
            print(f"   {i}. Model: {row['model_name']}")
            print(f"      Dataset: {row['dataset_name']}")
            print(f"      CER: {row['cer_percent']:.2f}%")
            print(f"      Character Accuracy: {row['character_accuracy_calculated']:.2f}%")
            
            # ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á
            if 'ground_truth' in df.columns and 'ocr_text' in df.columns:
                print(f"      Ground Truth: {str(row['ground_truth'])[:80]}...")
                print(f"      OCR Result: {str(row['ocr_text'])[:80]}...")
            elif 'reference_normalized' in df.columns and 'hypothesis_normalized' in df.columns:
                print(f"      Reference: {str(row['reference_normalized'])[:80]}...")
                print(f"      Hypothesis: {str(row['hypothesis_normalized'])[:80]}...")
            print("")
    else:
        print(f"\n‚úÖ ‡πÑ‡∏°‡πà‡∏û‡∏ö Character Accuracy ‡∏ï‡∏¥‡∏î‡∏•‡∏ö")
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≤‡∏° model ‡πÅ‡∏•‡∏∞ dataset
    print(f"\nüìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ï‡∏≤‡∏° Model:")
    model_stats = df.groupby('model_name')['character_accuracy_calculated'].agg(['min', 'max', 'mean', 'count']).round(2)
    print(model_stats)
    
    print(f"\nüìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ï‡∏≤‡∏° Dataset:")
    dataset_stats = df.groupby('dataset_name')['character_accuracy_calculated'].agg(['min', 'max', 'mean', 'count']).round(2)
    print(dataset_stats)
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≤‡∏° Model + Dataset
    print(f"\nüìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ï‡∏≤‡∏° Model x Dataset:")
    combined_stats = df.groupby(['model_name', 'dataset_name'])['character_accuracy_calculated'].agg(['min', 'max', 'mean', 'count']).round(2)
    print(combined_stats)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà CER > 100%
    high_cer = df[df['cer_percent'] > 100]
    
    if len(high_cer) > 0:
        print(f"\nüö® ‡∏û‡∏ö CER > 100%: {len(high_cer):,} ‡∏Å‡∏£‡∏ì‡∏µ")
        print(f"   - CER ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: {high_cer['cer_percent'].max():.2f}%")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
        extreme_cases = high_cer.nlargest(3, 'cer_percent')
        print(f"\nüî• 3 ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà CER ‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î:")
        for i, (idx, row) in enumerate(extreme_cases.iterrows(), 1):
            print(f"   {i}. CER: {row['cer_percent']:.2f}% | Accuracy: {row['character_accuracy_calculated']:.2f}%")
            print(f"      Model: {row['model_name']} | Dataset: {row['dataset_name']}")
            if 'ground_truth' in df.columns and 'ocr_text' in df.columns:
                print(f"      Truth: '{str(row['ground_truth'])[:50]}...'")
                print(f"      OCR: '{str(row['ocr_text'])[:50]}...'")
            print("")
    else:
        print(f"\n‚úÖ ‡πÑ‡∏°‡πà‡∏û‡∏ö CER > 100%")
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß
    print(f"\nüìà ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á CER:")
    bins = [0, 10, 20, 30, 50, 100, 200, float('inf')]
    labels = ['0-10%', '10-20%', '20-30%', '30-50%', '50-100%', '100-200%', '>200%']
    df['cer_range'] = pd.cut(df['cer_percent'], bins=bins, labels=labels, right=False)
    cer_distribution = df['cer_range'].value_counts().sort_index()
    
    for range_label, count in cer_distribution.items():
        percentage = (count / len(df)) * 100
        print(f"   {range_label}: {count:,} ‡∏Å‡∏£‡∏ì‡∏µ ({percentage:.1f}%)")
    
    return df

if __name__ == "__main__":
    df_analyzed = analyze_negative_accuracy()
